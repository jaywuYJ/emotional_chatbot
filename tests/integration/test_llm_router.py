#!/usr/bin/env python3
"""
æµ‹è¯•LLMè·¯ç”±å™¨åŠŸèƒ½
éªŒè¯å¤šç§LLMæä¾›å•†çš„æ”¯æŒ
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from backend.modules.llm.llm_router import LLMRouter
from backend.modules.llm.providers.base_provider import LLMMessage
from backend.models import ChatRequest
from backend.services.chat_service import ChatService

async def test_llm_router():
    """æµ‹è¯•LLMè·¯ç”±å™¨åŸºæœ¬åŠŸèƒ½"""
    print("=" * 80)
    print("æµ‹è¯•LLMè·¯ç”±å™¨")
    print("=" * 80)
    
    try:
        # åˆå§‹åŒ–è·¯ç”±å™¨
        router = LLMRouter()
        
        # æ˜¾ç¤ºå½“å‰æä¾›å•†ä¿¡æ¯
        current_info = router.get_current_provider_info()
        print(f"ğŸ¤– å½“å‰æä¾›å•†: {current_info['name']}")
        print(f"ğŸ“‹ æ¨¡å‹: {current_info.get('model', 'unknown')}")
        print(f"âœ… å¯ç”¨: {current_info['available']}")
        
        # åˆ—å‡ºæ‰€æœ‰æä¾›å•†
        print(f"\nğŸ“Š æ‰€æœ‰æä¾›å•†çŠ¶æ€:")
        providers = router.list_available_providers()
        for provider in providers:
            status = "âœ…" if provider['available'] else "âŒ"
            error = f" ({provider.get('error', '')})" if not provider['available'] and provider.get('error') else ""
            print(f"   {status} {provider['name']}: {provider['model']}{error}")
        
        # æµ‹è¯•èŠå¤©åŠŸèƒ½
        if current_info['available']:
            print(f"\nğŸ’¬ æµ‹è¯•èŠå¤©åŠŸèƒ½...")
            messages = [
                LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„AIåŠ©æ‰‹ã€‚"),
                LLMMessage(role="user", content="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚")
            ]
            
            response = await router.chat_completion(messages, max_tokens=100)
            print(f"ğŸ‘¤ ç”¨æˆ·: ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚")
            print(f"ğŸ¤– AI: {response.content}")
            print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {response.model}")
            
            if response.usage:
                print(f"ğŸ“ˆ Tokenä½¿ç”¨: {response.usage}")
        else:
            print(f"\nâš ï¸  æ²¡æœ‰å¯ç”¨çš„LLMæä¾›å•†ï¼Œè·³è¿‡èŠå¤©æµ‹è¯•")
        
    except Exception as e:
        print(f"âŒ LLMè·¯ç”±å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def test_chat_service_with_router():
    """æµ‹è¯•èŠå¤©æœåŠ¡ä½¿ç”¨æ–°è·¯ç”±å™¨"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•èŠå¤©æœåŠ¡é›†æˆ")
    print("=" * 80)
    
    try:
        # åˆå§‹åŒ–èŠå¤©æœåŠ¡
        chat_service = ChatService()
        
        # æµ‹è¯•å¯¹è¯
        user_id = "router_test_user"
        session_id = "router_test_session"
        
        test_messages = [
            "ä½ å¥½ï¼Œæˆ‘æƒ³æµ‹è¯•ä¸€ä¸‹æ–°çš„LLMè·¯ç”±å™¨åŠŸèƒ½",
            "ä½ ç°åœ¨ä½¿ç”¨çš„æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ",
            "èƒ½å‘Šè¯‰æˆ‘ä¸€äº›å…³äºRAGæŠ€æœ¯çš„ä¿¡æ¯å—ï¼Ÿ"
        ]
        
        for i, message in enumerate(test_messages, 1):
            print(f"\nğŸ”„ ç¬¬{i}è½®å¯¹è¯:")
            print(f"ğŸ‘¤ ç”¨æˆ·: {message}")
            
            request = ChatRequest(
                message=message,
                user_id=user_id,
                session_id=session_id
            )
            
            response = await chat_service.chat(request, use_memory_system=True)
            print(f"ğŸ¤– AI: {response.response}")
            print(f"ğŸ˜Š æƒ…æ„Ÿ: {response.emotion}")
            
            # æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦æ­£ç¡®ä¼ é€’
            if i > 1 and "è·¯ç”±å™¨" in message and "è·¯ç”±å™¨" not in response.response:
                print(f"âš ï¸  å¯èƒ½å­˜åœ¨ä¸Šä¸‹æ–‡ä¼ é€’é—®é¢˜")
            elif i > 1:
                print(f"âœ… ä¸Šä¸‹æ–‡ä¼ é€’æ­£å¸¸")
        
    except Exception as e:
        print(f"âŒ èŠå¤©æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def test_provider_switching():
    """æµ‹è¯•æä¾›å•†åˆ‡æ¢åŠŸèƒ½"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•æä¾›å•†åˆ‡æ¢")
    print("=" * 80)
    
    try:
        router = LLMRouter()
        
        # è·å–å¯ç”¨æä¾›å•†åˆ—è¡¨
        providers = router.list_available_providers()
        available_providers = [p for p in providers if p['available']]
        
        if len(available_providers) < 2:
            print(f"âš ï¸  åªæœ‰ {len(available_providers)} ä¸ªå¯ç”¨æä¾›å•†ï¼Œè·³è¿‡åˆ‡æ¢æµ‹è¯•")
            return
        
        print(f"ğŸ“‹ å¯ç”¨æä¾›å•†: {[p['name'] for p in available_providers]}")
        
        # æµ‹è¯•åˆ‡æ¢åˆ°ä¸åŒçš„æä¾›å•†
        for provider in available_providers:
            provider_name = provider['name']
            print(f"\nğŸ”„ åˆ‡æ¢åˆ°: {provider_name}")
            
            success = router.switch_provider(provider_name)
            if success:
                current_info = router.get_current_provider_info()
                print(f"âœ… åˆ‡æ¢æˆåŠŸ: {current_info['name']} ({current_info['model']})")
                
                # æµ‹è¯•ç®€å•å¯¹è¯
                messages = [LLMMessage(role="user", content="ä½ å¥½")]
                response = await router.chat_completion(messages, max_tokens=50)
                print(f"ğŸ’¬ æµ‹è¯•å›å¤: {response.content[:100]}...")
            else:
                print(f"âŒ åˆ‡æ¢å¤±è´¥")
        
    except Exception as e:
        print(f"âŒ æä¾›å•†åˆ‡æ¢æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def test_ollama_specific():
    """ä¸“é—¨æµ‹è¯•OllamaåŠŸèƒ½"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•Ollamaæœ¬åœ°æ¨¡å‹")
    print("=" * 80)
    
    try:
        from backend.modules.llm.providers.ollama_provider import OllamaProvider
        
        # æµ‹è¯•Ollamaè¿æ¥
        config = {
            'base_url': 'http://localhost:11434',
            'model': 'qwen2.5:8b',
            'temperature': 0.7,
            'max_tokens': 1000
        }
        
        provider = OllamaProvider(config)
        
        print(f"ğŸ” æ£€æŸ¥Ollamaå¯ç”¨æ€§...")
        if provider.is_available():
            print(f"âœ… OllamaæœåŠ¡å¯ç”¨")
            
            # è·å–æ¨¡å‹åˆ—è¡¨
            models = provider.list_models()
            print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {models}")
            
            # æµ‹è¯•å¯¹è¯
            messages = [
                LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"),
                LLMMessage(role="user", content="ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±ã€‚")
            ]
            
            print(f"ğŸ’¬ æµ‹è¯•Ollamaå¯¹è¯...")
            response = await provider.chat_completion(messages)
            print(f"ğŸ¤– Ollamaå›å¤: {response.content}")
            print(f"ğŸ“Š Tokenä½¿ç”¨: {response.usage}")
        else:
            print(f"âŒ OllamaæœåŠ¡ä¸å¯ç”¨")
            print(f"ğŸ’¡ è¯·ç¡®ä¿Ollamaå·²å¯åŠ¨å¹¶ä¸”æ¨¡å‹å·²ä¸‹è½½:")
            print(f"   1. å¯åŠ¨Ollama: ollama serve")
            print(f"   2. ä¸‹è½½æ¨¡å‹: ollama pull qwen2.5:8b")
        
    except Exception as e:
        print(f"âŒ Ollamaæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ LLMè·¯ç”±å™¨åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    
    await test_ollama_specific()
    await test_llm_router()
    await test_chat_service_with_router()
    await test_provider_switching()
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æµ‹è¯•å®Œæˆ")
    print("=" * 80)
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("1. ğŸ  æœ¬åœ°å¼€å‘æ¨èä½¿ç”¨Ollamaï¼ˆå…è´¹ã€éšç§ã€å¿«é€Ÿï¼‰")
    print("2. â˜ï¸  ç”Ÿäº§ç¯å¢ƒå¯ä»¥é…ç½®å¤šä¸ªæä¾›å•†å®ç°æ•…éšœè½¬ç§»")
    print("3. ğŸ”§ é€šè¿‡ç¯å¢ƒå˜é‡è°ƒæ•´æä¾›å•†ä¼˜å…ˆçº§")
    print("4. ğŸ“Š ç›‘æ§å„æä¾›å•†çš„å¯ç”¨æ€§å’Œå“åº”æ—¶é—´")

if __name__ == "__main__":
    asyncio.run(main())