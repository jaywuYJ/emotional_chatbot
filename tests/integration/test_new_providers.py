#!/usr/bin/env python3
"""
æµ‹è¯•æ–°çš„LLMæä¾›å•†ï¼ˆDeepSeekã€Claudeã€Geminiï¼‰
"""
import os
import sys

# æ‰‹åŠ¨åŠ è½½config.envæ–‡ä»¶
def load_env_file(file_path):
    """æ‰‹åŠ¨åŠ è½½.envæ–‡ä»¶"""
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key] = value

# åŠ è½½ç¯å¢ƒå˜é‡
load_env_file('config.env')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from backend.modules.llm.llm_router import LLMRouter
from backend.modules.llm.providers.base_provider import LLMMessage

def test_providers():
    print("=" * 60)
    print("æµ‹è¯•æ–°çš„LLMæä¾›å•†")
    print("=" * 60)
    
    # åˆå§‹åŒ–è·¯ç”±å™¨
    try:
        router = LLMRouter()
        print(f"âœ… LLMè·¯ç”±å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ LLMè·¯ç”±å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æä¾›å•†
    print("\nğŸ“‹ å¯ç”¨çš„æä¾›å•†:")
    providers = router.list_available_providers()
    for provider in providers:
        status = "âœ…" if provider['available'] else "âŒ"
        print(f"   {status} {provider['name']}: {provider['model']}")
        if not provider['available'] and 'error' in provider:
            print(f"      é”™è¯¯: {provider['error']}")
    
    # è·å–å½“å‰æä¾›å•†ä¿¡æ¯
    current_provider = router.get_current_provider_info()
    print(f"\nğŸ¯ å½“å‰é€‰æ‹©çš„æä¾›å•†: {current_provider['name']}")
    if current_provider['available']:
        print(f"   æ¨¡å‹: {current_provider.get('model', 'unknown')}")
    else:
        print("   çŠ¶æ€: ä¸å¯ç”¨")
        return
    
    # æµ‹è¯•å¯¹è¯
    print(f"\nğŸ’¬ æµ‹è¯•ä¸ {current_provider['name']} çš„å¯¹è¯...")
    
    messages = [
        LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"),
        LLMMessage(role="user", content="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚")
    ]
    
    try:
        response = router.chat_completion(messages=messages, max_tokens=100)
        print(f"ğŸ‘¤ ç”¨æˆ·: ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚")
        print(f"ğŸ¤– AI: {response.content}")
        print(f"ğŸ“Š Tokenä½¿ç”¨: {response.usage}")
        print("âœ… å¯¹è¯æµ‹è¯•æˆåŠŸ")
    except Exception as e:
        print(f"âŒ å¯¹è¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # æµ‹è¯•æ•…éšœè½¬ç§»
    print(f"\nğŸ”„ æµ‹è¯•æ•…éšœè½¬ç§»åŠŸèƒ½...")
    available_providers = [p for p in providers if p['available']]
    
    if len(available_providers) > 1:
        print(f"å‘ç° {len(available_providers)} ä¸ªå¯ç”¨æä¾›å•†ï¼Œæµ‹è¯•åˆ‡æ¢...")
        
        for provider in available_providers[1:2]:  # åªæµ‹è¯•ç¬¬äºŒä¸ªæä¾›å•†
            try:
                success = router.switch_provider(provider['name'])
                if success:
                    print(f"âœ… æˆåŠŸåˆ‡æ¢åˆ°: {provider['name']}")
                    
                    # æµ‹è¯•æ–°æä¾›å•†
                    response = router.chat_completion(messages=messages, max_tokens=50)
                    print(f"ğŸ¤– {provider['name']}: {response.content[:100]}...")
                else:
                    print(f"âŒ åˆ‡æ¢åˆ° {provider['name']} å¤±è´¥")
            except Exception as e:
                print(f"âŒ æµ‹è¯• {provider['name']} å¤±è´¥: {e}")
    else:
        print("åªæœ‰ä¸€ä¸ªå¯ç”¨æä¾›å•†ï¼Œæ— æ³•æµ‹è¯•æ•…éšœè½¬ç§»")
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 60)

def show_config_help():
    """æ˜¾ç¤ºé…ç½®å¸®åŠ©"""
    print("\n" + "=" * 60)
    print("é…ç½®è¯´æ˜")
    print("=" * 60)
    
    print("\nğŸ”§ è¦å¯ç”¨æ–°çš„æä¾›å•†ï¼Œè¯·åœ¨ config.env ä¸­é…ç½®ç›¸åº”çš„APIå¯†é’¥ï¼š")
    
    print("\n1. SiliconFlow (æ¨èï¼ŒAPIèšåˆå¹³å°ï¼Œæ”¯æŒå¤šç§æ¨¡å‹):")
    print("   SILICONFLOW_API_KEY=your_siliconflow_api_key")
    print("   è·å–åœ°å€: https://siliconflow.cn/")
    print("   æ”¯æŒæ¨¡å‹: Qwen/Qwen2.5-7B-Instruct, meta-llama/Meta-Llama-3-8B-Instruct ç­‰")
    print("   Embedding: BAAI/bge-m3")
    
    print("\n2. DeepSeek (æ€§ä»·æ¯”é«˜):")
    print("   DEEPSEEK_API_KEY=your_deepseek_api_key")
    print("   è·å–åœ°å€: https://platform.deepseek.com/")
    
    print("\n3. Claude (è´¨é‡å¾ˆé«˜):")
    print("   CLAUDE_API_KEY=your_claude_api_key")
    print("   è·å–åœ°å€: https://console.anthropic.com/")
    
    print("\n4. Gemini (å…è´¹é¢åº¦å¤§):")
    print("   GEMINI_API_KEY=your_gemini_api_key")
    print("   è·å–åœ°å€: https://makersuite.google.com/app/apikey")
    
    print("\nğŸ’¡ æç¤º:")
    print("   - ä¼˜å…ˆçº§æ•°å­—è¶Šå°ï¼Œä¼˜å…ˆçº§è¶Šé«˜")
    print("   - ç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜ä¸”å¯ç”¨çš„æä¾›å•†")
    print("   - å¦‚æœå½“å‰æä¾›å•†å¤±è´¥ï¼Œä¼šè‡ªåŠ¨æ•…éšœè½¬ç§»åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨æä¾›å•†")

if __name__ == "__main__":
    test_providers()
    show_config_help()